#!/usr/local/bin/python3
# -*- coding: utf-8 -*-

# Author: Doug Rudolph (https://github.com/11)
# Date: Jan. 5, 2025
# Description: LLM CLI

# Setup:
# You will need create these environment variables for each service you'd like to work with
#   - `OPENAI_API_KEY='<your-openai-key>'`
#   - `ANTHROPIC_API_KEY='<your-anthropic-key>'`
#   - `GEMINI_API_KEY='<your-gemini-key>'`


import os
import json
import sys
from urllib import request, error
from argparse import ArgumentParser


#
# UTILITY FUNCTIONS
#


def create_argparser() -> ArgumentParser:
    parser = ArgumentParser(prog='llm', description='LLM CLI')
    subparsers = parser.add_subparsers(dest='command', required=True)

    # env command
    env_cmd = subparsers.add_parser('env', help='List found LLM environment variables')

    # chat command and its arguments
    prompt_cmd = subparsers.add_parser('chat', help='Send a prompt to an LLM model')
    prompt_cmd.add_argument(
        'prompt',
        type=str,
        help='Input prompt to send to model'
    )

    prompt_cmd.add_argument(
        '-m',
        '--model',
        type=str,
        choices=['gpt-4', 'gpt-4o-mini', 'gpt-4o'],
        default='gpt-4o-mini',
        help='AI model - defaults to `gpt-3.5-turbo`'
    )

    prompt_cmd.add_argument(
        '-o',
        '--output',
        type=str,
        choices=['json', 'text'],
        default='text',
        help='Output format - defaults to `text`'
    )

    prompt_cmd.add_argument(
        '-t',
        '--tokens',
        type=int,
        default=2048,
        help='Max amount of tokens- defaults to `2048`'
    )

    prompt_cmd.add_argument(
        '-v',
        '--variance',
        type=float,
        choices=[.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0],
        default=0.7,
        help='Model\'s  temperature - defaults to `0.7`'
    )

    prompt_cmd.add_argument(
        '-r',
        '--role',
        type=str,
        choices=['user', 'system'],
        default='user',
        help='Model role - defaults to `user`'
    )

    return parser


def fetch(url: str, method: str = 'POST', headers: dict | None = None, body: dict | None = None):
    """ Make a post HTTP request """

    if body:
        body = json.dumps(body).encode('utf-8')

    req = request.Request(
        url,
        method=method,
        headers=headers or {},
        data=body
    )

    try:
        with request.urlopen(req) as res:
            data = res.read().decode('utf-8')
            try:
                return json.loads(data)
            except json.JSONDecodeError:
                return data
    except error.HTTPError as err:
        return f'HTTP Error: {err.code}, {err.reason}'
    except error.URLError as err:
        return f'URL Error: {err.reason}'


#
# LLM REQUEST FUNCTIONS
#


def openai_request(prompt: str, model: str, output: str, tokens: int, variance: float, role: str):
    """ OpenAI chat request """

    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', None)
    if OPENAI_API_KEY is None:
        print('Could not find `OPENAI_API_KEY` environment variable', file=sys.stderr)
        return


    url = 'https://api.openai.com/v1/chat/completions'
    res = fetch(
        url,
        method='POST',
        headers={
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {OPENAI_API_KEY}',
        },
        body={
            'model': model,
            'messages': [{ 'role': role, 'content': prompt }],
            'temperature': variance,
            'max_completion_tokens': tokens,
            'response_format': { 'type': output },
        },
    )

    result = res['choices'][0]['message']['content']
    print(result)


#
# CLI COMMAND FUNCTIONS
#


def list_found_environment_variables():
    """ List out `found` and `missing` LLM API environment variables """
    services = {
        'Anthropic': os.environ.get('ANTHROPIC_API_KEY', None) is not None,
        'OpenAI': os.environ.get('OPENAI_API_KEY', None) is not None,
        'Gemini': os.environ.get('GEMINI_API_KEY', None) is not None,
    }

    padding = max(len(service) for service in services)
    for service, valid in services.items():
        status = '\033[92mfound\033[0m' if valid else '\033[91mmissing\033[0m'
        print(f'{service:<{padding}}\t{status}')


if __name__ == '__main__':
    parser = create_argparser()
    args = parser.parse_args()

    if args.command == 'env':
        list_found_environment_variables()
    elif args.command == 'chat':
        params = vars(args)
        del params['command']

        openai_request(**params)
