#!/usr/local/bin/python3
# -*- coding: utf-8 -*-

# Author: Doug Rudolph (https://github.com/11)
# Date: Jan. 5, 2025
# Description: LLM CLI


import os
import json
import sys
from enum import Enum
from urllib import request, parse, error
from argparse import ArgumentParser, BooleanOptionalAction
from typing import List, TypedDict


def fetch(url: str, method: str ='POST', headers: dict | None = None, body: dict | None = None):
    if body:
        body = json.dumps(body).encode('utf-8')

    req = request.Request(
        url,
        method=method,
        headers=headers or {},
        data=body
    )

    try:
        with request.urlopen(req) as res:
            data = res.read().decode('utf-8')
            try:
                return json.loads(data)
            except json.JSONDecodeError:
                return data
    except error.HTTPError as err:
        return f'HTTP Error: {err.code}, {err.reason}'
    except error.URLError as err:
        return f'URL Error: {err.reason}'


def openai_request(prompt: str, model: str, output: str, tokens: int, variance: float, role: str):
    OPENAI_API_KEY = os.environ['OPENAI_API_KEY']
    if OPENAI_API_KEY is None:
        print('Could not find `OPENAI_API_KEY` environment variable', file=sys.stderr)
        return


    url = 'https://api.openai.com/v1/chat/completions'
    res = fetch(
        url,
        method='POST',
        headers={
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {OPENAI_API_KEY}',
        },
        body={
            'model': model,
            'messages': [{ 'role': role, 'content': prompt }],
            'temperature': variance,
            'max_completion_tokens': tokens,
            'response_format': { 'type': output },
        },
    )

    return res['choices'][0]['message']['content']


if __name__ == '__main__':
    parser = ArgumentParser(prog='llm', description='LLM CLI')

    parser.add_argument(
        'prompt',
        type=str,
        help='Input to a prompt to model'
    )

    parser.add_argument(
        '-m',
        '--model',
        type=str,
        choices=['gpt-3.5-turbo', 'gpt-4', 'gpt-4o-mini', 'gpt-4o'],
        default='gpt-3.5-turbo',
        help='AI model - defaults to `gpt-3.5-turbo`'
    )

    parser.add_argument(
        '-o',
        '--output',
        type=str,
        choices=['json', 'text'],
        default='text',
        help='Output format - defaults to `text`'
    )

    parser.add_argument(
        '-t',
        '--tokens',
        type=int,
        default=2048,
        help='Max amount of tokens- defaults to `2048`'
    )

    parser.add_argument(
        '-v',
        '--variance',
        type=float,
        choices=[.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0],
        default=0.7,
        help='Model\'s  temperature - defaults to `0.7`'
    )

    parser.add_argument(
        '-r',
        '--role',
        type=str,
        choices=['user', 'system'],
        default='user',
        help='Model role - defaults to `user`'
    )

    args = vars(parser.parse_args())
    result = openai_request(**args)
    print(result)
